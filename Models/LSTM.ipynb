{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "qbGiyKbnW7Iw",
   "metadata": {
    "id": "qbGiyKbnW7Iw"
   },
   "outputs": [],
   "source": [
    "#ANAS\n",
    "from google.colab import drive\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Bidirectional,  Dense, Dropout , BatchNormalization\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.layers import Layer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "import re\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.metrics import accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "iPWFERUGmsVz",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iPWFERUGmsVz",
    "outputId": "d2ffc01d-3fa2-4090-911c-ce67ee0db42e"
   },
   "outputs": [],
   "source": [
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "gKmpwbpXW65_",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gKmpwbpXW65_",
    "outputId": "81b76eda-fe1c-4c46-f661-a0c29505ac0a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\WIN-10\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\WIN-10\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\WIN-10\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\WIN-10\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt_tab')\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c1Mkidj6vqr",
   "metadata": {
    "id": "8c1Mkidj6vqr"
   },
   "outputs": [],
   "source": [
    "class LSTMCell(Layer):\n",
    "    def __init__(self, units, **kwargs):\n",
    "        super(LSTMCell, self).__init__(**kwargs)\n",
    "        self.units = units\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        input_dim = input_shape[-1]\n",
    "        self.Wx = self.add_weight(shape=(input_dim, self.units * 4),\n",
    "                                  initializer=\"glorot_uniform\",\n",
    "                                  name=\"Wx\")\n",
    "        self.Wh = self.add_weight(shape=(self.units, self.units * 4),\n",
    "                                  initializer=\"glorot_uniform\",\n",
    "                                  name=\"Wh\")\n",
    "        self.bias = self.add_weight(shape=(self.units * 4,),\n",
    "                                     initializer=\"zeros\",\n",
    "                                     name=\"bias\")\n",
    "        self.built = True\n",
    "\n",
    "    def call(self, inputs, states):\n",
    "        h_tm1, c_tm1 = states\n",
    "        z = tf.matmul(inputs, self.Wx) + tf.matmul(h_tm1, self.Wh) + self.bias\n",
    "        z0, z1, z2, z3 = tf.split(z, num_or_size_splits=4, axis=1)\n",
    "        i = tf.sigmoid(z0)\n",
    "        f = tf.sigmoid(z1)\n",
    "        o = tf.sigmoid(z3)\n",
    "        g = tf.tanh(z2)\n",
    "        c_t = f * c_tm1 + i * g\n",
    "        h_t = o * tf.tanh(c_t)\n",
    "        return h_t, [h_t, c_t]\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(LSTMCell, self).get_config()\n",
    "        config.update({\"units\": self.units})\n",
    "        return config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1fRsctOj60HC",
   "metadata": {
    "id": "1fRsctOj60HC"
   },
   "outputs": [],
   "source": [
    "class LSTM(Layer):\n",
    "    def __init__(self, units, return_sequences=False, **kwargs):\n",
    "        super(LSTM, self).__init__(**kwargs)\n",
    "        self.units = units\n",
    "        self.return_sequences = return_sequences\n",
    "        self.cell = LSTMCell(units)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        batch_size = tf.shape(inputs)[0]\n",
    "        time_steps = tf.shape(inputs)[1]\n",
    "        h = tf.zeros((batch_size, self.units))\n",
    "        c = tf.zeros((batch_size, self.units))\n",
    "\n",
    "        def step_fn(prev_states, x_t):\n",
    "            h, c = prev_states\n",
    "            h, [h, c] = self.cell(x_t, [h, c])\n",
    "            return h, c\n",
    "\n",
    "        h_states = tf.TensorArray(tf.float32, size=time_steps)\n",
    "\n",
    "        for t in tf.range(time_steps):\n",
    "            h, c = step_fn([h, c], inputs[:, t, :])\n",
    "            h_states = h_states.write(t, h)\n",
    "\n",
    "        if self.return_sequences:\n",
    "            return tf.transpose(h_states.stack(), [1, 0, 2])\n",
    "        else:\n",
    "            return h\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(LSTM, self).get_config()\n",
    "        config.update({\n",
    "            \"units\": self.units,\n",
    "            \"return_sequences\": self.return_sequences\n",
    "        })\n",
    "        return config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "HkgaC5GqW6mW",
   "metadata": {
    "id": "HkgaC5GqW6mW"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/content/drive/MyDrive/NN_Data/train.csv\")\n",
    "df = df.iloc[:, 1:]\n",
    "\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "\n",
    "    text = re.sub(r'http\\S+|www\\S+|@\\S+|#\\S+', '', text)\n",
    "    text = \"\".join([char for char in text if char.isalpha() or char.isspace()])\n",
    "\n",
    "\n",
    "    tokens = word_tokenize(text.lower())\n",
    "\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "    tokens = [wordnet_lemmatizer.lemmatize(word) for word in tokens]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "df['Discussion'] = df['Discussion'].apply(preprocess_text)\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ccDlQspTXUct",
   "metadata": {
    "id": "ccDlQspTXUct"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                  Sports\n",
      "1                    STEM\n",
      "2                    STEM\n",
      "3                  Sports\n",
      "4                Politics\n",
      "               ...       \n",
      "24984              Sports\n",
      "24985    Market & Economy\n",
      "24986    Market & Economy\n",
      "24987            Politics\n",
      "24988               Media\n",
      "Name: Category, Length: 24989, dtype: object\n",
      "0        1\n",
      "1        4\n",
      "2        4\n",
      "3        1\n",
      "4        0\n",
      "        ..\n",
      "24984    1\n",
      "24985    3\n",
      "24986    3\n",
      "24987    0\n",
      "24988    2\n",
      "Name: Category, Length: 24989, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df['Category'])\n",
    "category_mapping = {'Politics': 0, 'Sports': 1, 'Media': 2, 'Market & Economy': 3, 'STEM': 4}\n",
    "df['Category'] = df['Category'].map(category_mapping)\n",
    "print(df['Category'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ZUYQGInvXVCJ",
   "metadata": {
    "id": "ZUYQGInvXVCJ"
   },
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X = tfidf_vectorizer.fit_transform(df['Discussion']).toarray()\n",
    "\n",
    "y = np.array(df['Category'])\n",
    "# label_encoder = LabelEncoder()\n",
    "# y = label_encoder.fit_transform(y)\n",
    "y = to_categorical(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "FyAnnkycXgUJ",
   "metadata": {
    "id": "FyAnnkycXgUJ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19991\n",
      "5000\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "print(X_train.shape[0])\n",
    "print(X_train.shape[1])\n",
    "X_train = X_train.reshape(X_train.shape[0], 1, X_train.shape[1])\n",
    "X_test = X_test.reshape(X_test.shape[0], 1, X_test.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ehqtnXNTXhaI",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ehqtnXNTXhaI",
    "outputId": "e84a364c-4ca2-4c3e-c29f-2cf2d0aeb78c"
   },
   "outputs": [],
   "source": [
    "model_lstm = tf.keras.Sequential([\n",
    "\n",
    "    LSTM(128, return_sequences=False, input_shape=(X_train.shape[1], X_train.shape[2])),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dropout(0.3),\n",
    "\n",
    "    # LSTM(64, return_sequences=False),\n",
    "    # tf.keras.layers.Dropout(0.2),\n",
    "\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(y_train.shape[1], activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b717a656-ee5d-4698-b630-d0b94af43157",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b717a656-ee5d-4698-b630-d0b94af43157",
    "outputId": "2a42331b-1c82-44f8-fc02-50d014eeef89",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "313/313 [==============================] - 9s 13ms/step - loss: 1.0185 - accuracy: 0.6079 - val_loss: 1.2262 - val_accuracy: 0.6331\n",
      "Epoch 2/30\n",
      "313/313 [==============================] - 5s 15ms/step - loss: 0.7004 - accuracy: 0.7306 - val_loss: 0.8923 - val_accuracy: 0.6531\n",
      "Epoch 3/30\n",
      "313/313 [==============================] - 3s 11ms/step - loss: 0.5752 - accuracy: 0.7748 - val_loss: 1.0175 - val_accuracy: 0.6401\n",
      "Epoch 4/30\n",
      "313/313 [==============================] - 3s 11ms/step - loss: 0.4873 - accuracy: 0.8081 - val_loss: 1.1639 - val_accuracy: 0.6297\n",
      "Epoch 5/30\n",
      "313/313 [==============================] - 3s 11ms/step - loss: 0.4150 - accuracy: 0.8335 - val_loss: 1.2932 - val_accuracy: 0.6305\n",
      "aaa\n",
      "157/157 - 0s - loss: 0.8923 - accuracy: 0.6531 - 247ms/epoch - 2ms/step\n",
      "Bidirectional LSTM Test Accuracy: 65.31%\n"
     ]
    }
   ],
   "source": [
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "model_lstm.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "history = model_lstm.fit(X_train, y_train, epochs=30, batch_size=64,\n",
    "                         validation_data=(X_test, y_test),\n",
    "                         callbacks=[early_stopping])\n",
    "print(\"aaa\")\n",
    "test_loss, test_accuracy = model_lstm.evaluate(X_test, y_test, verbose=2)\n",
    "print(f'Bidirectional LSTM Test Accuracy: {test_accuracy * 100:.2f}%')\n",
    "\n",
    "model_lstm.save('tfidf_lstm_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "v3pgQcP0OzMg",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "v3pgQcP0OzMg",
    "outputId": "e419cb71-d863-436f-8e8f-2fbdbfd52b3d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 128)               2626048   \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 128)              512       \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 128)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 5)                 325       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,635,141\n",
      "Trainable params: 2,634,885\n",
      "Non-trainable params: 256\n",
      "_________________________________________________________________\n",
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n"
     ]
    }
   ],
   "source": [
    "model_lstm.summary()\n",
    "keras.utils.plot_model(model_lstm, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "18a00998-799e-4050-bcc4-0346d056c621",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "18a00998-799e-4050-bcc4-0346d056c621",
    "outputId": "914b615b-0bca-4c49-c57d-8162c96a38dc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "330/330 [==============================] - 1s 1ms/step\n",
      "   SampleID  Category\n",
      "0         1         3\n",
      "1         2         0\n",
      "2         3         1\n",
      "3         4         4\n",
      "4         5         3\n",
      "5         6         0\n",
      "6         7         1\n",
      "7         8         3\n",
      "8         9         4\n",
      "9        10         3\n"
     ]
    }
   ],
   "source": [
    "#testing\n",
    "test_data = pd.read_csv(\"test.csv\")\n",
    "test_data['Discussion'] = test_data['Discussion'].apply(preprocess_text)\n",
    "pred = tfidf_vectorizer.transform(test_data['Discussion']).toarray()\n",
    "\n",
    "pred = pred.reshape(pred.shape[0], 1, pred.shape[1])\n",
    "pred = model_lstm.predict(pred)\n",
    "test_predictions = np.argmax(pred, axis=1)\n",
    "result = pd.DataFrame({\n",
    "    \"SampleID\": test_data[\"SampleID\"].values,\n",
    "    \"Category\": test_predictions\n",
    "})\n",
    "print(result.head(10))\n",
    "#result.to_csv('/content/drive/MyDrive/NN_Data/output.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8dd0ddc3-d349-4eb0-9a68-33a1429a3738",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "781/781 [==============================] - 2s 2ms/step\n",
      "Accuracy on new data: 78.87%\n",
      "Predictions saved to 'new_data_with_predictions.csv'\n"
     ]
    }
   ],
   "source": [
    "model = load_model('tfidf_lstm_model.h5')\n",
    "\n",
    "new_data = pd.read_csv(\"train.csv\")\n",
    "\n",
    "new_data['Discussion'] = new_data['Discussion'].apply(preprocess_text)\n",
    "X_new = tfidf_vectorizer.transform(new_data['Discussion']).toarray()\n",
    "\n",
    "X_new = X_new.reshape(X_new.shape[0], 1, X_new.shape[1])\n",
    "\n",
    "predictions = model.predict(X_new)\n",
    "predicted_classes = np.argmax(predictions, axis=1)\n",
    "\n",
    "new_data['Category'] = new_data['Category'].map(category_mapping)\n",
    "\n",
    "accuracy = accuracy_score(new_data['Category'], predicted_classes)\n",
    "print(f\"Accuracy on new data: {accuracy * 100:.2f}%\")\n",
    "new_data['Predicted_Class'] = predicted_classes\n",
    "new_data.to_csv(\"new_data_with_predictions.csv\", index=False)\n",
    "\n",
    "print(\"Predictions saved to 'new_data_with_predictions.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa9ff51-fccf-4042-a0ab-549ae5883c4d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
